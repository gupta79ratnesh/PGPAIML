{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:33:21.367021Z",
     "start_time": "2018-03-09T22:33:12.637484+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, LearningRateScheduler\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K \n",
    "from keras.models import load_model\n",
    "from math import ceil \n",
    "import numpy as np \n",
    "from termcolor import colored\n",
    "\n",
    "from mn_model import mn_model\n",
    "from face_generator import BatchGenerator\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "\n",
    "import scipy.misc as sm\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # choose gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:33:21.378011Z",
     "start_time": "2018-03-09T22:33:21.368987+08:00"
    }
   },
   "outputs": [],
   "source": [
    "img_height = 512\n",
    "img_width = 512\n",
    "img_channels = 3\n",
    "\n",
    "n_classes = 2 \n",
    "class_names = [\"background\",\"face\"]\n",
    "\n",
    "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n",
    "aspect_ratios = [[0.5, 1.0, 2.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [0.5, 1.0, 2.0],\n",
    "                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n",
    "two_boxes_for_ar1 = True\n",
    "limit_boxes = True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
    "coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
    "normalize_coords = True\n",
    "\n",
    "det_model_path = \"./models/\" \n",
    "train_data = 'wider_train_small.npy'\n",
    "test_data = 'wider_val_small.npy'\n",
    "data_path = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:33:35.285228Z",
     "start_time": "2018-03-09T22:33:21.379013+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height, Width, Channels : 512 512 3\n",
      "Freezing classification layers\n",
      "\u001b[32mclassification layers freezed\u001b[0m\n",
      "loading classification weights\n",
      "\u001b[32mclassification weights ./base_models/mobilenet_1_0_224_tf.h5 loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# build the keras model\n",
    "# this model is not retrained, we are doing it from scratch \n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model, model_layer, img_input, predictor_sizes = mn_model(image_size=(img_height, img_width, img_channels), \n",
    "                                                                      n_classes = n_classes,\n",
    "                                                                      min_scale = None, \n",
    "                                                                      max_scale = None, \n",
    "                                                                      scales = scales, \n",
    "                                                                      aspect_ratios_global = None, \n",
    "                                                                      aspect_ratios_per_layer = aspect_ratios, \n",
    "                                                                      two_boxes_for_ar1= two_boxes_for_ar1, \n",
    "                                                                      limit_boxes=limit_boxes, \n",
    "                                                                      variances= variances, \n",
    "                                                                      coords=coords, \n",
    "                                                                      normalize_coords=normalize_coords)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "print (\"Freezing classification layers\")\n",
    "#Freeze layers\n",
    "for layer_key in model_layer:\n",
    "  if('detection'  not in layer_key): #prefix detection to freeze layers which does not have detection\n",
    "    model_layer[layer_key].trainable = False\n",
    "print (colored(\"classification layers freezed\", 'green'))\n",
    "\n",
    "# for layer in model.layers:\n",
    "#   print (colored(layer.name, 'blue'))\n",
    "#   print (colored(layer.trainable, 'green'))\n",
    "\n",
    "print (\"loading classification weights\")\n",
    "classification_model = './base_models/mobilenet_1_0_224_tf.h5'\n",
    "model.load_weights(classification_model,  by_name= True)\n",
    "\n",
    "print (colored( ('classification weights %s loaded' % classification_model), 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:36:54.861936Z",
     "start_time": "2018-03-09T22:33:35.286188+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Total number of training samples = 128\n",
      "VALIDATION DATA\n",
      "Total number of validation samples = 60\n"
     ]
    }
   ],
   "source": [
    "# setting up taining \n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "#Adam\n",
    "base_lr = 0.002\n",
    "adam = Adam(lr=base_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay = 0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=2, n_neg_min=0, alpha=1.0, beta = 1.0)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "\n",
    "ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
    "                                img_width=img_width,\n",
    "                                n_classes=n_classes, \n",
    "                                predictor_sizes=predictor_sizes,\n",
    "                                min_scale=None,\n",
    "                                max_scale=None,\n",
    "                                scales=scales,\n",
    "                                aspect_ratios_global=None,\n",
    "                                aspect_ratios_per_layer=aspect_ratios,\n",
    "                                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                limit_boxes=limit_boxes,\n",
    "                                variances=variances,\n",
    "                                pos_iou_threshold=0.5,\n",
    "                                neg_iou_threshold=0.2,\n",
    "                                coords=coords,\n",
    "                                normalize_coords=normalize_coords)\n",
    "\n",
    "train_dataset = BatchGenerator(images_path=train_data, \n",
    "                               image_set_path=data_path,\n",
    "                               include_classes='all', \n",
    "                               box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "print (\"TRAINING DATA\")\n",
    "\n",
    "train_dataset.parse_xml(\n",
    "                  annotations_path=train_data,\n",
    "                  image_set_path='None',\n",
    "                  image_set='None',\n",
    "                  classes = class_names, \n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False, \n",
    "                  debug = False)\n",
    "\n",
    "train_generator = train_dataset.generate(\n",
    "                 batch_size=batch_size,\n",
    "                 train=True,\n",
    "                 ssd_box_encoder=ssd_box_encoder,\n",
    "                 equalize=True,\n",
    "                 brightness=(0.5,2,0.5),\n",
    "                 flip=0.5,\n",
    "                 translate=((0, 20), (0, 30), 0.5),\n",
    "                 scale=(0.75, 1.2, 0.5),\n",
    "                 crop=False,\n",
    "                 #random_crop = (img_height,img_width,1,3), \n",
    "                 random_crop=False,\n",
    "                 resize=(img_height, img_width),\n",
    "                 #resize=False,\n",
    "                 gray=False,\n",
    "                 limit_boxes=True,\n",
    "                 include_thresh=0.4,\n",
    "                 diagnostics=False)\n",
    "\n",
    "n_train_samples = train_dataset.get_n_samples()\n",
    "\n",
    "print (\"Total number of training samples = {}\".format(n_train_samples))\n",
    "\n",
    "\n",
    "print (\"VALIDATION DATA\")\n",
    "\n",
    "val_dataset = BatchGenerator(images_path=test_data, include_classes='all', \n",
    "                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "\n",
    "val_dataset.parse_xml(\n",
    "                  annotations_path=test_data,\n",
    "                  image_set_path='None',\n",
    "                  image_set='None',\n",
    "                  classes = class_names, \n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False, \n",
    "                  debug = False)\n",
    "\n",
    "\n",
    "val_generator = val_dataset.generate(\n",
    "                 batch_size=batch_size,\n",
    "                 train=True,\n",
    "                 ssd_box_encoder=ssd_box_encoder,\n",
    "                 equalize=False,\n",
    "                 brightness=False,\n",
    "                 flip=False,\n",
    "                 translate=False,\n",
    "                 scale=False,\n",
    "                 crop=False,\n",
    "                 #random_crop = (img_height,img_width,1,3), \n",
    "                 random_crop=False, \n",
    "                 resize=(img_height, img_width), \n",
    "                 #resize=False, \n",
    "                 gray=False,\n",
    "                 limit_boxes=True,\n",
    "                 include_thresh=0.4,\n",
    "                 diagnostics=False)\n",
    "\n",
    "n_val_samples = val_dataset.get_n_samples()\n",
    "\n",
    "print (\"Total number of validation samples = {}\".format(n_val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T14:40:49.615390Z",
     "start_time": "2018-03-09T22:36:54.863941+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starting...\n",
      "Epoch 1/10\n",
      "lr remains 0.0020000000949949026\n",
      "4/4 [==============================] - 45s 11s/step - loss: 0.1855 - val_loss: 0.2156\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21559, saving model to ./models/ssd_mobilenet_face_epoch_01_loss0.2156.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b5a4b1aa6ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                               \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                               validation_steps = ceil(n_val_samples/batch_size))\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_model_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ssd_mobilenet_weights_epoch_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    440\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0msymbolic_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \"\"\"\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# now start the training \n",
    "\n",
    "def scheduler(epoch):\n",
    "  if epoch%10==0 and epoch!=0:\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, lr*.95)\n",
    "    print(\"lr changed to {}\".format(lr*.95))\n",
    "  else: \n",
    "    print(\"lr remains {}\".format(K.get_value(model.optimizer.lr)))\n",
    "\n",
    "  return K.get_value(model.optimizer.lr)\n",
    "\n",
    "lr_schedule = LearningRateScheduler(scheduler)\n",
    "\n",
    "plateau = ReduceLROnPlateau(monitor='val_loss', factor = 0.3, patience =4, epsilon=0.001, cooldown=0)\n",
    "tensorboard = TensorBoard(log_dir='./logs/trial1/', histogram_freq=1, batch_size=16, write_graph=True, write_grads=True, \n",
    "                          write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=100)\n",
    "model_checkpoint =  ModelCheckpoint(det_model_path + 'ssd_mobilenet_face_epoch_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)\n",
    "\n",
    "\n",
    "history = model.fit_generator(generator = train_generator,\n",
    "                              steps_per_epoch = ceil(n_train_samples/batch_size)*2,\n",
    "                              epochs = num_epochs,\n",
    "                              callbacks = [model_checkpoint, lr_schedule, early_stopping],                      \n",
    "                              validation_data = val_generator,\n",
    "                              validation_steps = ceil(n_val_samples/batch_size))\n",
    "\n",
    "model.save_weights(det_model_path + 'ssd_mobilenet_weights_epoch_{}.h5'.format(epochs))\n",
    "\n",
    "print (\"model and weight files saved at : \" + det_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T15:11:52.562338Z",
     "start_time": "2018-03-09T23:11:52.383835+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights ./models/ssd_mobilenet_face_epoch_25_loss0.0916.h5 loaded\n"
     ]
    }
   ],
   "source": [
    "model_path = './models/'\n",
    "model_name = 'ssd_mobilenet_face_epoch_25_loss0.0916.h5'\n",
    "\n",
    "model.load_weights(model_path + model_name,  by_name= True)\n",
    "\n",
    "print (colored('weights %s loaded' % (model_path + model_name), 'green'))\n",
    "\n",
    "def save_bb(path, filename, results, prediction=True):\n",
    "  \n",
    "  # print filename\n",
    "\n",
    "  img = image.load_img(filename, target_size=(img_height, img_width))\n",
    "  img = image.img_to_array(img)\n",
    "\n",
    "  filename = filename.split(\"/\")[-1]\n",
    "\n",
    "  if(not prediction):\n",
    "    filename = filename[:-4] + \"_gt\" + \".jpg\"\n",
    "\n",
    "  #fig,currentAxis = plt.subplots(1)\n",
    "  currentAxis = plt.gca()\n",
    "\n",
    " # Get detections with confidence higher than 0.6.\n",
    "  colors = plt.cm.hsv(np.linspace(0, 1, 25)).tolist()\n",
    "  color_code = min(len(results), 16)\n",
    "  print (colored(\"total number of bbs: %d\" % len(results), \"yellow\"))\n",
    "  for result in results:\n",
    "    # Parse the outputs.\n",
    "\n",
    "    if(prediction):\n",
    "      det_label = result[0]\n",
    "      det_conf = result[1]\n",
    "      det_xmin = result[2]\n",
    "      det_xmax = result[3]\n",
    "      det_ymin = result[4]\n",
    "      det_ymax = result[5]\n",
    "    else :\n",
    "      det_label = result[0]\n",
    "      det_xmin = result[1]\n",
    "      det_xmax = result[2]\n",
    "      det_ymin = result[3]\n",
    "      det_ymax = result[4]\n",
    "\n",
    "    xmin = int(det_xmin)\n",
    "    ymin = int(det_ymin)\n",
    "    xmax = int(det_xmax)\n",
    "    ymax = int(det_ymax)\n",
    "\n",
    "    if(prediction):\n",
    "      score = det_conf\n",
    "    \n",
    "    plt.imshow(img / 255.)\n",
    "    \n",
    "    label = int(int(det_label))\n",
    "    label_name = class_names[label]\n",
    "    # print label_name \n",
    "    # print label\n",
    "\n",
    "    if(prediction):\n",
    "      display_txt = '{:0.2f}'.format(score)\n",
    "    else:\n",
    "      display_txt = '{}'.format(label_name)\n",
    "\n",
    "      \n",
    "    # print (xmin, ymin, ymin, ymax)\n",
    "    coords = (xmin, ymin), (xmax-xmin), (ymax-ymin)\n",
    "    color_code = color_code-1 \n",
    "    color = colors[color_code]\n",
    "    currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.2})\n",
    "\n",
    "  # y 轴不可见\n",
    "  currentAxis.axes.get_yaxis().set_visible(False)\n",
    "  # x 轴不可见\n",
    "  currentAxis.axes.get_xaxis().set_visible(False)\n",
    "  plt.savefig(path + filename, bbox_inches='tight')\n",
    "\n",
    "  print ('saved' , path + filename)\n",
    "\n",
    "  plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T15:42:25.277298Z",
     "start_time": "2018-03-09T23:42:06.631484+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "now predicting...\n",
      "total number of bbs: 11\n",
      "saved ./output_test/35_Basketball_Basketball_35_361.jpg\n",
      "total number of bbs: 16\n",
      "saved ./output_test/35_Basketball_Basketball_35_361_gt.jpg\n",
      "total number of bbs: 1\n",
      "saved ./output_test/24_Soldier_Firing_Soldier_Firing_24_887.jpg\n",
      "total number of bbs: 1\n",
      "saved ./output_test/24_Soldier_Firing_Soldier_Firing_24_887_gt.jpg\n",
      "total number of bbs: 7\n",
      "saved ./output_test/38_Tennis_Tennis_38_683.jpg\n",
      "total number of bbs: 7\n",
      "saved ./output_test/38_Tennis_Tennis_38_683_gt.jpg\n",
      "total number of bbs: 3\n",
      "saved ./output_test/5_Car_Accident_Accident_5_244.jpg\n",
      "total number of bbs: 10\n",
      "saved ./output_test/5_Car_Accident_Accident_5_244_gt.jpg\n",
      "total number of bbs: 0\n",
      "saved ./output_test/23_Shoppers_Shoppers_23_450.jpg\n",
      "total number of bbs: 3\n",
      "saved ./output_test/23_Shoppers_Shoppers_23_450_gt.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/0_Parade_marchingband_1_822.jpg\n",
      "total number of bbs: 6\n",
      "saved ./output_test/0_Parade_marchingband_1_822_gt.jpg\n",
      "total number of bbs: 2\n",
      "saved ./output_test/35_Basketball_Basketball_35_185.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/35_Basketball_Basketball_35_185_gt.jpg\n",
      "total number of bbs: 6\n",
      "saved ./output_test/3_Riot_Riot_3_725.jpg\n",
      "total number of bbs: 16\n",
      "saved ./output_test/3_Riot_Riot_3_725_gt.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/3_Riot_Riot_3_604.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/3_Riot_Riot_3_604_gt.jpg\n",
      "total number of bbs: 1\n",
      "saved ./output_test/13_Interview_Interview_Sequences_13_864.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/13_Interview_Interview_Sequences_13_864_gt.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/0_Parade_Parade_0_353.jpg\n",
      "total number of bbs: 11\n",
      "saved ./output_test/0_Parade_Parade_0_353_gt.jpg\n",
      "total number of bbs: 1\n",
      "saved ./output_test/13_Interview_Interview_Sequences_13_270.jpg\n",
      "total number of bbs: 1\n",
      "saved ./output_test/13_Interview_Interview_Sequences_13_270_gt.jpg\n",
      "total number of bbs: 4\n",
      "saved ./output_test/52_Photographers_photographertakingphoto_52_316.jpg\n",
      "total number of bbs: 3\n",
      "saved ./output_test/52_Photographers_photographertakingphoto_52_316_gt.jpg\n",
      "total number of bbs: 2\n",
      "saved ./output_test/44_Aerobics_Aerobics_44_809.jpg\n",
      "total number of bbs: 3\n",
      "saved ./output_test/44_Aerobics_Aerobics_44_809_gt.jpg\n",
      "total number of bbs: 2\n",
      "saved ./output_test/20_Family_Group_Family_Group_20_193.jpg\n",
      "total number of bbs: 3\n",
      "saved ./output_test/20_Family_Group_Family_Group_20_193_gt.jpg\n",
      "total number of bbs: 9\n",
      "saved ./output_test/2_Demonstration_Demonstration_Or_Protest_2_441.jpg\n",
      "total number of bbs: 18\n",
      "saved ./output_test/2_Demonstration_Demonstration_Or_Protest_2_441_gt.jpg\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "test_size = 16\n",
    "test_generator = val_dataset.generate(\n",
    "                 batch_size=test_size,\n",
    "                 train=False,\n",
    "                 ssd_box_encoder=ssd_box_encoder,\n",
    "                 equalize=False,\n",
    "                 brightness=False,\n",
    "                 flip=False,\n",
    "                 translate=False,\n",
    "                 scale=False,\n",
    "                 crop=False,\n",
    "                 #random_crop = (img_height,img_width,1,3), \n",
    "                 random_crop=False, \n",
    "                 resize=(img_height, img_width), \n",
    "                 #resize=False,\n",
    "                 gray=False,\n",
    "                 limit_boxes=True,\n",
    "                 include_thresh=0.4,\n",
    "                 diagnostics=False)\n",
    "\n",
    "print (colored(\"done.\", \"green\"))\n",
    "\n",
    "print (colored(\"now predicting...\", \"yellow\"))\n",
    "\n",
    "_CONF = 0.60 \n",
    "_IOU = 0.15\n",
    "\n",
    "for i in range(test_size):\n",
    "  X, y, filenames = next(test_generator)\n",
    "\n",
    "  y_pred = model.predict(X)\n",
    "\n",
    "\n",
    "  y_pred_decoded = decode_y2(y_pred,\n",
    "                             confidence_thresh=_CONF,\n",
    "                            iou_threshold=_IOU,\n",
    "                            top_k='all',\n",
    "                            input_coords=coords,\n",
    "                            normalize_coords=normalize_coords,\n",
    "                            img_height=img_height,\n",
    "                            img_width=img_width)\n",
    "\n",
    "\n",
    "  np.set_printoptions(suppress=True)\n",
    "\n",
    "  save_bb(\"./output_test/\", filenames[i], y_pred_decoded[i])\n",
    "  save_bb(\"./output_test/\", filenames[i], y[i], prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
