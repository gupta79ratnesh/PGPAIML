{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning - Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandrashekarCYoga/PG-AIML/blob/master/Transfer_Learning_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LildFBrEBUxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwD7CBz5BcJC",
        "colab_type": "code",
        "outputId": "890aa7bf-a308-4441-97c8-15d4ebb8a14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q77KmVS9Bmys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path = \"/content/drive/My Drive/Colab Notebooks/Dog Breed Data/data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wOSm9ZIBn2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'train.zip', 'r') as z:\n",
        "  z.extractall()\n",
        "\n",
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'test.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z_yPRCcPYsf",
        "colab_type": "text"
      },
      "source": [
        "## Dataset description\n",
        "The data basically contains the target columns and the image ID of the file that the particular target belongs to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq06hEMJBrrd",
        "colab_type": "code",
        "outputId": "69bfbe44-8010-4c1b-a7ed-7962c2ab79cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "labels = pd.read_csv(project_path+\"labels.csv\")\n",
        "labels.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>breed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
              "      <td>boston_bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
              "      <td>dingo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
              "      <td>pekinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
              "      <td>bluetick</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
              "      <td>golden_retriever</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 id             breed\n",
              "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
              "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
              "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
              "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
              "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXgInIhdB6f0",
        "colab_type": "code",
        "outputId": "82784481-913e-4720-daf7-3735bde75980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "breed_count = labels['breed'].value_counts()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scottish_deerhound      126\n",
              "maltese_dog             117\n",
              "afghan_hound            116\n",
              "entlebucher             115\n",
              "bernese_mountain_dog    114\n",
              "                       ... \n",
              "komondor                 67\n",
              "golden_retriever         67\n",
              "brabancon_griffon        67\n",
              "eskimo_dog               66\n",
              "briard                   66\n",
              "Name: breed, Length: 120, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AaoThJRCEg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "targets = pd.Series(labels['breed'])\n",
        "one_hot = pd.get_dummies(targets, sparse = True)\n",
        "one_hot_labels = np.asarray(one_hot)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEU4WEykB63J",
        "colab_type": "code",
        "outputId": "fbb0f407-3455-49c1-ee45-a87bc94020a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "one_hot_labels[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osNG32AICKIO",
        "colab_type": "code",
        "outputId": "6cf98f6f-4279-4c44-ae15-bebba14f6a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "keras.utils.to_categorical(le.fit_transform(labels['breed']))[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJlMqC0SCv9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows=128\n",
        "img_cols=128\n",
        "num_channel=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMAoWyGiPmmS",
        "colab_type": "text"
      },
      "source": [
        "tqdm is a package in python which shows nice and neat loading interface for iterators and other formats while unzipping them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AEcCOZACyQG",
        "colab_type": "code",
        "outputId": "902ced77-6732-41b4-c42f-f9d1426cb213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import cv2\n",
        "x_feature = []\n",
        "y_feature = []\n",
        "\n",
        "i = 0 # initialisation\n",
        "for f, img in tqdm(labels.values): # f for format ,jpg\n",
        "    train_img = cv2.imread('./train/{}.jpg'.format(f),0)\n",
        "    label = one_hot_labels[i]\n",
        "    train_img_resize = cv2.resize(train_img, (img_rows, img_cols)) \n",
        "    x_feature.append(train_img_resize)\n",
        "    y_feature.append(label)\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10222/10222 [00:13<00:00, 769.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh5EFEusPurU",
        "colab_type": "text"
      },
      "source": [
        "### Point to note\n",
        "\n",
        "Post reading the vector from the images the default format is that of the (*number of images*, *img height*, *img width*) but keras expects that the image also contain the number of channels which we accomplish by using the expand dims method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q1gPWl-C9yj",
        "colab_type": "code",
        "outputId": "e7bef896-fa11-4983-8e9c-8df6d1d266c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train_data = np.array(x_feature, np.float32) / 255.   # /= 255 for normalisation\n",
        "print (x_train_data.shape)\n",
        "x_train_data = np.expand_dims(x_train_data, axis = 3) # for keras to given input to Conv2D layer\n",
        "print (x_train_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10222, 128, 128)\n",
            "(10222, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAyX5gScDCda",
        "colab_type": "code",
        "outputId": "cd7b41ad-42e3-473d-d722-b61c27455515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_data = np.array(y_feature)\n",
        "\n",
        "y_train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10222, 120)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quKtw87CDfLy",
        "colab_type": "code",
        "outputId": "f86844d9-d995-4b28-f196-15f3f51a7e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_data, y_train_data, test_size=0.2, random_state=2)\n",
        "print (x_train.shape)\n",
        "print (x_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8177, 128, 128, 1)\n",
            "(2045, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4319vLr6Dhr_",
        "colab_type": "code",
        "outputId": "3ef43c48-b29c-4a9f-e74a-8101b6e912e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "submission = pd.read_csv(project_path+'sample_submission.csv')\n",
        "test_img = submission['id']\n",
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>affenpinscher</th>\n",
              "      <th>afghan_hound</th>\n",
              "      <th>african_hunting_dog</th>\n",
              "      <th>airedale</th>\n",
              "      <th>american_staffordshire_terrier</th>\n",
              "      <th>appenzeller</th>\n",
              "      <th>australian_terrier</th>\n",
              "      <th>basenji</th>\n",
              "      <th>basset</th>\n",
              "      <th>beagle</th>\n",
              "      <th>bedlington_terrier</th>\n",
              "      <th>bernese_mountain_dog</th>\n",
              "      <th>black-and-tan_coonhound</th>\n",
              "      <th>blenheim_spaniel</th>\n",
              "      <th>bloodhound</th>\n",
              "      <th>bluetick</th>\n",
              "      <th>border_collie</th>\n",
              "      <th>border_terrier</th>\n",
              "      <th>borzoi</th>\n",
              "      <th>boston_bull</th>\n",
              "      <th>bouvier_des_flandres</th>\n",
              "      <th>boxer</th>\n",
              "      <th>brabancon_griffon</th>\n",
              "      <th>briard</th>\n",
              "      <th>brittany_spaniel</th>\n",
              "      <th>bull_mastiff</th>\n",
              "      <th>cairn</th>\n",
              "      <th>cardigan</th>\n",
              "      <th>chesapeake_bay_retriever</th>\n",
              "      <th>chihuahua</th>\n",
              "      <th>chow</th>\n",
              "      <th>clumber</th>\n",
              "      <th>cocker_spaniel</th>\n",
              "      <th>collie</th>\n",
              "      <th>curly-coated_retriever</th>\n",
              "      <th>dandie_dinmont</th>\n",
              "      <th>dhole</th>\n",
              "      <th>dingo</th>\n",
              "      <th>doberman</th>\n",
              "      <th>...</th>\n",
              "      <th>norwegian_elkhound</th>\n",
              "      <th>norwich_terrier</th>\n",
              "      <th>old_english_sheepdog</th>\n",
              "      <th>otterhound</th>\n",
              "      <th>papillon</th>\n",
              "      <th>pekinese</th>\n",
              "      <th>pembroke</th>\n",
              "      <th>pomeranian</th>\n",
              "      <th>pug</th>\n",
              "      <th>redbone</th>\n",
              "      <th>rhodesian_ridgeback</th>\n",
              "      <th>rottweiler</th>\n",
              "      <th>saint_bernard</th>\n",
              "      <th>saluki</th>\n",
              "      <th>samoyed</th>\n",
              "      <th>schipperke</th>\n",
              "      <th>scotch_terrier</th>\n",
              "      <th>scottish_deerhound</th>\n",
              "      <th>sealyham_terrier</th>\n",
              "      <th>shetland_sheepdog</th>\n",
              "      <th>shih-tzu</th>\n",
              "      <th>siberian_husky</th>\n",
              "      <th>silky_terrier</th>\n",
              "      <th>soft-coated_wheaten_terrier</th>\n",
              "      <th>staffordshire_bullterrier</th>\n",
              "      <th>standard_poodle</th>\n",
              "      <th>standard_schnauzer</th>\n",
              "      <th>sussex_spaniel</th>\n",
              "      <th>tibetan_mastiff</th>\n",
              "      <th>tibetan_terrier</th>\n",
              "      <th>toy_poodle</th>\n",
              "      <th>toy_terrier</th>\n",
              "      <th>vizsla</th>\n",
              "      <th>walker_hound</th>\n",
              "      <th>weimaraner</th>\n",
              "      <th>welsh_springer_spaniel</th>\n",
              "      <th>west_highland_white_terrier</th>\n",
              "      <th>whippet</th>\n",
              "      <th>wire-haired_fox_terrier</th>\n",
              "      <th>yorkshire_terrier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000621fb3cbb32d8935728e48679680e</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00102ee9d8eb90812350685311fe5890</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0012a730dfa437f5f3613fb75efcd4ce</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>001510bc8570bbeee98c8d80c8a95ec1</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>001a5f3114548acdefa3d4da05474c2e</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 121 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 id  ...  yorkshire_terrier\n",
              "0  000621fb3cbb32d8935728e48679680e  ...           0.008333\n",
              "1  00102ee9d8eb90812350685311fe5890  ...           0.008333\n",
              "2  0012a730dfa437f5f3613fb75efcd4ce  ...           0.008333\n",
              "3  001510bc8570bbeee98c8d80c8a95ec1  ...           0.008333\n",
              "4  001a5f3114548acdefa3d4da05474c2e  ...           0.008333\n",
              "\n",
              "[5 rows x 121 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvz-RxsREU4i",
        "colab_type": "code",
        "outputId": "6f3a479a-d7b5-4751-8444-0961daf201d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test_feature = []\n",
        "\n",
        "i = 0 # initialisation\n",
        "for f in tqdm(test_img.values): # f for format ,jpg\n",
        "    img = cv2.imread('./test/{}.jpg'.format(f), 0)\n",
        "    img_resize = cv2.resize(img, (img_rows, img_cols)) \n",
        "    x_test_feature.append(img_resize)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10357/10357 [00:13<00:00, 783.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu87cElSEaxr",
        "colab_type": "code",
        "outputId": "556cbc88-5059-480c-af25-fd68a1a11639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_test_data = np.array(x_test_feature, np.float32) / 255. \n",
        "print (x_test_data.shape)\n",
        "x_test_data = np.expand_dims(x_test_data, axis = 3)\n",
        "print (x_test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10357, 128, 128)\n",
            "(10357, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azygSOCTEdJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential  # initial NN\n",
        "from keras.layers import Dense, Dropout # construct each layer\n",
        "from keras.layers import Conv2D # swipe across the image by 1\n",
        "from keras.layers import MaxPooling2D # swipe across by pool size\n",
        "from keras.layers import Flatten, GlobalAveragePooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwbqQjJWEeiC",
        "colab_type": "code",
        "outputId": "326ac1f2-469e-4359-a320-778cd3405197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (5,5), activation ='relu', input_shape = (img_rows, img_cols, num_channel))) \n",
        "model.add(MaxPooling2D(pool_size=3))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Flatten()) \n",
        "# fully connected layer \n",
        "model.add(Dense(units = 500, activation = 'relu')) \n",
        "model.add(Dropout(0.8))\n",
        "# output layer\n",
        "model.add(Dense(units = 120, activation = 'softmax')) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdFjIcSWEkw1",
        "colab_type": "code",
        "outputId": "851b7785-732f-4fea-87d2-d9b567052de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(lr=0.01)\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 124, 124, 32)      832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 41, 41, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 41, 41, 64)        32832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 20, 20, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 128)       65664     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               1600500   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 120)               60120     \n",
            "=================================================================\n",
            "Total params: 1,833,804\n",
            "Trainable params: 1,833,804\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCGlnw5dEm0S",
        "colab_type": "code",
        "outputId": "c6111886-eb42-4a24-c8ba-2c7b9e5bb85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "batch_size = 128\n",
        "nb_epochs = 10\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=nb_epochs, \n",
        "                    validation_data=(x_val, y_val),\n",
        "                    initial_epoch=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 8177 samples, validate on 2045 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "8177/8177 [==============================] - 15s 2ms/step - loss: 4.8339 - acc: 0.0106 - val_loss: 4.7881 - val_acc: 0.0088\n",
            "Epoch 2/10\n",
            "8177/8177 [==============================] - 7s 828us/step - loss: 4.7818 - acc: 0.0103 - val_loss: 4.7893 - val_acc: 0.0088\n",
            "Epoch 3/10\n",
            "8177/8177 [==============================] - 7s 824us/step - loss: 4.7783 - acc: 0.0120 - val_loss: 4.7916 - val_acc: 0.0117\n",
            "Epoch 4/10\n",
            "8177/8177 [==============================] - 7s 835us/step - loss: 4.7780 - acc: 0.0106 - val_loss: 4.7911 - val_acc: 0.0117\n",
            "Epoch 5/10\n",
            "8177/8177 [==============================] - 7s 831us/step - loss: 4.7770 - acc: 0.0113 - val_loss: 4.7926 - val_acc: 0.0117\n",
            "Epoch 6/10\n",
            "8177/8177 [==============================] - 7s 827us/step - loss: 4.7770 - acc: 0.0113 - val_loss: 4.7930 - val_acc: 0.0117\n",
            "Epoch 7/10\n",
            "8177/8177 [==============================] - 7s 827us/step - loss: 4.7771 - acc: 0.0125 - val_loss: 4.7931 - val_acc: 0.0117\n",
            "Epoch 8/10\n",
            "8177/8177 [==============================] - 7s 827us/step - loss: 4.7772 - acc: 0.0105 - val_loss: 4.7931 - val_acc: 0.0088\n",
            "Epoch 9/10\n",
            "8177/8177 [==============================] - 7s 830us/step - loss: 4.7771 - acc: 0.0127 - val_loss: 4.7930 - val_acc: 0.0117\n",
            "Epoch 10/10\n",
            "8177/8177 [==============================] - 7s 826us/step - loss: 4.7771 - acc: 0.0115 - val_loss: 4.7936 - val_acc: 0.0117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifvy1mpSErGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "vgg = vgg16.VGG16(include_top=False, weights='imagenet', \n",
        "                                     input_shape=input_shape)\n",
        "output = vgg.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "vgg_model = Model(vgg.input, output)\n",
        "\n",
        "# Instantiate the model with the pre-trained weights (no top)\n",
        "base_model= VGG16(weights=(project_path+'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'),\n",
        "                 include_top=False, pooling='avg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihqa-IT5GVHK",
        "colab_type": "code",
        "outputId": "7f861c56-9b03-446a-c1aa-122e1c5c5cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpUCUjfkQNyf",
        "colab_type": "text"
      },
      "source": [
        "### Point to note\n",
        "\n",
        "The GAP2D layer is used to extract the average value of all filters in that layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkILLbB2QZ5b",
        "colab_type": "text"
      },
      "source": [
        "## Bottleneck feature extraction\n",
        "The below approach details on how to use the bottle neck features from a dataset (ie) use an existing framework to predict/process an image and then use that information to build additional layers/custom prediction approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVKDVTNCFULO",
        "colab_type": "code",
        "outputId": "a89f1f25-c566-40a2-9e1c-ea3da5cb21d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "X_arr = []\n",
        "for id, label in tqdm(labels.values):\n",
        "    X_arr.append(base_model.predict(\n",
        "        preprocess_input(\n",
        "            np.expand_dims(\n",
        "                image.img_to_array(\n",
        "                    image.load_img('./train/'+id+'.jpg', target_size=[128,128])), axis=0)))[0])\n",
        "X = pd.DataFrame(X_arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10222/10222 [02:33<00:00, 66.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhsYvXgcH2xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential([\n",
        "    Dense(1024, input_shape=(512,)),\n",
        "    Activation('relu'),\n",
        "    Dense(256, input_shape=(512,)),\n",
        "    Activation('relu'),\n",
        "    Dense(120),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_train = X\n",
        "y_train = y_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwmBatLnQqbt",
        "colab_type": "text"
      },
      "source": [
        "# Trainable Layers\n",
        "Certain layers in a network can be frozen or unfrozen based on the need of the data and the model to ensure that the loss is used to ideally adapt the weights of the existing architechture as well.\n",
        "\n",
        "<p> Below is an example of such a method/process </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlkMKJkvKjd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = vgg.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "vgg_model = Model(vgg.input, output)\n",
        "\n",
        "vgg_model.trainable = False\n",
        "for layer in vgg_model.layers:\n",
        "  if layer.str.contains(\"block5\"):\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "    \n",
        "  \n",
        "model = Sequential()\n",
        "model.add(vgg_model)\n",
        "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(120, activation='softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DqS-Hu8IMlB",
        "colab_type": "code",
        "outputId": "259cb9e1-6869-4f6f-e7af-7dacb519c1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, np.asarray(y_train), epochs=50, batch_size=100, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "10222/10222 [==============================] - 1s 104us/step - loss: 4.4309 - acc: 0.1487\n",
            "Epoch 2/50\n",
            "10222/10222 [==============================] - 1s 67us/step - loss: 2.2486 - acc: 0.4168\n",
            "Epoch 3/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 1.6072 - acc: 0.5650\n",
            "Epoch 4/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 1.1824 - acc: 0.6640\n",
            "Epoch 5/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.8678 - acc: 0.7467\n",
            "Epoch 6/50\n",
            "10222/10222 [==============================] - 1s 74us/step - loss: 0.5795 - acc: 0.8330\n",
            "Epoch 7/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.3914 - acc: 0.8894\n",
            "Epoch 8/50\n",
            "10222/10222 [==============================] - 1s 74us/step - loss: 0.2327 - acc: 0.9402\n",
            "Epoch 9/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.1253 - acc: 0.9708\n",
            "Epoch 10/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0830 - acc: 0.9842\n",
            "Epoch 11/50\n",
            "10222/10222 [==============================] - 1s 68us/step - loss: 0.0597 - acc: 0.9890\n",
            "Epoch 12/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0471 - acc: 0.9915\n",
            "Epoch 13/50\n",
            "10222/10222 [==============================] - 1s 69us/step - loss: 0.0433 - acc: 0.9927\n",
            "Epoch 14/50\n",
            "10222/10222 [==============================] - 1s 67us/step - loss: 0.0354 - acc: 0.9935\n",
            "Epoch 15/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.0478 - acc: 0.9891\n",
            "Epoch 16/50\n",
            "10222/10222 [==============================] - 1s 69us/step - loss: 0.1322 - acc: 0.9623\n",
            "Epoch 17/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.3159 - acc: 0.9035\n",
            "Epoch 18/50\n",
            "10222/10222 [==============================] - 1s 74us/step - loss: 0.3247 - acc: 0.9008\n",
            "Epoch 19/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.2464 - acc: 0.9248\n",
            "Epoch 20/50\n",
            "10222/10222 [==============================] - 1s 70us/step - loss: 0.1356 - acc: 0.9583\n",
            "Epoch 21/50\n",
            "10222/10222 [==============================] - 1s 68us/step - loss: 0.0908 - acc: 0.9719\n",
            "Epoch 22/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.0858 - acc: 0.9740\n",
            "Epoch 23/50\n",
            "10222/10222 [==============================] - 1s 75us/step - loss: 0.0381 - acc: 0.9897\n",
            "Epoch 24/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0241 - acc: 0.9948\n",
            "Epoch 25/50\n",
            "10222/10222 [==============================] - 1s 70us/step - loss: 0.0223 - acc: 0.9950\n",
            "Epoch 26/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0764 - acc: 0.9782\n",
            "Epoch 27/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0949 - acc: 0.9755\n",
            "Epoch 28/50\n",
            "10222/10222 [==============================] - 1s 70us/step - loss: 0.1727 - acc: 0.9480\n",
            "Epoch 29/50\n",
            "10222/10222 [==============================] - 1s 77us/step - loss: 0.1880 - acc: 0.9442\n",
            "Epoch 30/50\n",
            "10222/10222 [==============================] - 1s 74us/step - loss: 0.1306 - acc: 0.9599\n",
            "Epoch 31/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.1095 - acc: 0.9651\n",
            "Epoch 32/50\n",
            "10222/10222 [==============================] - 1s 75us/step - loss: 0.0899 - acc: 0.9730\n",
            "Epoch 33/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.1160 - acc: 0.9674\n",
            "Epoch 34/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0705 - acc: 0.9793\n",
            "Epoch 35/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0508 - acc: 0.9840\n",
            "Epoch 36/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0802 - acc: 0.9771\n",
            "Epoch 37/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.0942 - acc: 0.9740\n",
            "Epoch 38/50\n",
            "10222/10222 [==============================] - 1s 75us/step - loss: 0.1131 - acc: 0.9673\n",
            "Epoch 39/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.0912 - acc: 0.9750\n",
            "Epoch 40/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0871 - acc: 0.9739\n",
            "Epoch 41/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0703 - acc: 0.9804\n",
            "Epoch 42/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.0963 - acc: 0.9721\n",
            "Epoch 43/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0675 - acc: 0.9802\n",
            "Epoch 44/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0776 - acc: 0.9791\n",
            "Epoch 45/50\n",
            "10222/10222 [==============================] - 1s 74us/step - loss: 0.1014 - acc: 0.9713\n",
            "Epoch 46/50\n",
            "10222/10222 [==============================] - 1s 71us/step - loss: 0.0781 - acc: 0.9772\n",
            "Epoch 47/50\n",
            "10222/10222 [==============================] - 1s 74us/step - loss: 0.0964 - acc: 0.9724\n",
            "Epoch 48/50\n",
            "10222/10222 [==============================] - 1s 73us/step - loss: 0.1201 - acc: 0.9681\n",
            "Epoch 49/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0895 - acc: 0.9755\n",
            "Epoch 50/50\n",
            "10222/10222 [==============================] - 1s 72us/step - loss: 0.0741 - acc: 0.9787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f21b00c7390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}